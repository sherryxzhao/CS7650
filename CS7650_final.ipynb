{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216f7f5b-3cd4-4c50-8112-d4e28a494a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import tqdm.notebook as tqdm\n",
    "import nltk\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57948a83-4996-4b41-b9f9-3954551849b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67835043-085d-48a9-88e6-a44b63162e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General util functions\n",
    "def make_dir_if_not_exists(directory):\n",
    "\tif not os.path.exists(directory):\n",
    "\t\tlogging.info(\"Creating new directory: {}\".format(directory))\n",
    "\t\tos.makedirs(directory)\n",
    "\n",
    "def print_list(l, K=None):\n",
    "\t# If K is given then only print first K\n",
    "\tfor i, e in enumerate(l):\n",
    "\t\tif i == K:\n",
    "\t\t\tbreak\n",
    "\t\tprint(e)\n",
    "\tprint()\n",
    "\n",
    "def remove_multiple_spaces(string):\n",
    "\treturn re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "def save_in_pickle(save_object, save_file):\n",
    "\twith open(save_file, \"wb\") as pickle_out:\n",
    "\t\tpickle.dump(save_object, pickle_out)\n",
    "\n",
    "def load_from_pickle(pickle_file):\n",
    "\twith open(pickle_file, \"rb\") as pickle_in:\n",
    "\t\treturn pickle.load(pickle_in)\n",
    "\n",
    "def save_in_txt(list_of_strings, save_file):\n",
    "\twith open(save_file, \"w\") as writer:\n",
    "\t\tfor line in list_of_strings:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\twriter.write(f\"{line}\\n\")\n",
    "\n",
    "def load_from_txt(txt_file):\n",
    "\twith open(txt_file, \"r\") as reader:\n",
    "\t\tall_lines = list()\n",
    "\t\tfor line in reader:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tall_lines.append(line)\n",
    "\t\treturn all_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a35f0-7948-4776-b92a-04f6e12ec833",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aee6aed7-7342-4e4b-b91e-31c8c1d4e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Constraint' # 'Constraint' # 'Rumor' # 'Political' # 'Liar'\n",
    "train_data = pd.read_csv(f'data/{dataset}_Train.csv')\n",
    "val_data = pd.read_csv(f'data/{dataset}_Validation.csv')\n",
    "test_data = pd.read_csv(f'data/{dataset}_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e58b3b62-a1eb-4df0-b995-2d2c912ad3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6420, 2140, 2140)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "844d5381-c90d-4c95-b2a4-85e6f667bce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10700, 26.983831775700935)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "tmp_df['length'] = tmp_df.content.apply(lambda x: len(x.split(' ')))\n",
    "len(tmp_df), tmp_df.length.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f25d4c0-a989-49b3-a967-2afa1e728d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'real': 0, 'fake': 1}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_data.label.unique()\n",
    "\n",
    "mapping_l2i = {l:i for i,l in enumerate(labels)}\n",
    "mapping_i2l = {i:l for i,l in enumerate(labels)}\n",
    "mapping_l2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06ea01b3-5047-4be8-a474-589089b41b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5233644859813084"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.label.value_counts().iloc[[0]].values[0]/len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b4ee104-8cfd-4f40-9013-73da4a7ae572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5233644859813084"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.label.value_counts().iloc[[0]].values[0]/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f60629d8-0f0c-4660-a903-bcc128ae2511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>2020 Indy 500 postponed from May to August due...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>T</td>\n",
       "      <td>With fears over the spread of Coronavirus moun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>U</td>\n",
       "      <td>\"It's Winter-Time For Capital Flow\" - Outbreak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>Cristiano Ronaldo will convert his hotels into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>T</td>\n",
       "      <td>Australia now has drive-thru coronavirus testi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>5020</td>\n",
       "      <td>F</td>\n",
       "      <td>Anybody in U.S. that wants a test (for the cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5021</th>\n",
       "      <td>5021</td>\n",
       "      <td>U</td>\n",
       "      <td>@HuffPost Result of China's long-run policy: #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5022</th>\n",
       "      <td>5022</td>\n",
       "      <td>F</td>\n",
       "      <td>Dawoodi Bohra youth were licking used utensils...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>5023</td>\n",
       "      <td>U</td>\n",
       "      <td>Wilbur Ross: Coronavirus Could ‘Accelerate The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5024</th>\n",
       "      <td>5024</td>\n",
       "      <td>U</td>\n",
       "      <td>More than 6,000 people are trapped on a cruise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5025 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 label                                            content\n",
       "0              0     T  2020 Indy 500 postponed from May to August due...\n",
       "1              1     T  With fears over the spread of Coronavirus moun...\n",
       "2              2     U  \"It's Winter-Time For Capital Flow\" - Outbreak...\n",
       "3              3     F  Cristiano Ronaldo will convert his hotels into...\n",
       "4              4     T  Australia now has drive-thru coronavirus testi...\n",
       "...          ...   ...                                                ...\n",
       "5020        5020     F  Anybody in U.S. that wants a test (for the cor...\n",
       "5021        5021     U  @HuffPost Result of China's long-run policy: #...\n",
       "5022        5022     F  Dawoodi Bohra youth were licking used utensils...\n",
       "5023        5023     U  Wilbur Ross: Coronavirus Could ‘Accelerate The...\n",
       "5024        5024     U  More than 6,000 people are trapped on a cruise...\n",
       "\n",
       "[5025 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b91fbfb-9bef-440a-8e5b-2eef2e00f401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'More than 6,000 people are trapped on a cruise ship in Italy after a woman was suspected of having the coronavirus'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data.iloc[[0]].content.values[0]\n",
    "train_data.iloc[[5024]].content.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ca7b28-05ca-49f2-83a1-fdc777931903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining torch dataset class for disaster tweet dataset\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.df['target'] = self.df.label.apply(lambda x: mapping_l2i[x])\n",
    "        self.df['id'] = list(range(len(df)))\n",
    "        self.df['content'] = self.df.content.apply(lambda x: ' '.join(x.split(' ')[:510]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192d9b51-69f6-474e-90cd-78c2e3646428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up train, validation, and testing datasets\n",
    "train_dataset = TweetDataset(train_data)\n",
    "val_dataset   = TweetDataset(val_data)\n",
    "test_dataset  = TweetDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3020a79a-0225-4567-b801-e53041ac1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_collate_fn(batch, tokenizer):\n",
    "  bert_vocab = tokenizer.get_vocab()\n",
    "  bert_pad_token = bert_vocab['[PAD]']\n",
    "  bert_unk_token = bert_vocab['[UNK]']\n",
    "  bert_cls_token = bert_vocab['[CLS]']\n",
    "\n",
    "  sentences, labels, masks = [], [], []\n",
    "  for data in batch:\n",
    "    tokenizer_output = tokenizer([data['content']])\n",
    "    # print(tokenizer_output)\n",
    "    tokenized_sent = tokenizer_output['input_ids'][0]\n",
    "    mask = tokenizer_output['attention_mask'][0]\n",
    "    \n",
    "    # print(tokenized_sent)\n",
    "    if len(tokenized_sent) > 512:\n",
    "        tokenized_sent = tokenized_sent[:511]+tokenized_sent[-1:]\n",
    "        mask = mask[:511]+mask[-1:]\n",
    "    \n",
    "    sentences.append(torch.tensor(tokenized_sent))\n",
    "    labels.append(torch.tensor(data['target']))\n",
    "    masks.append(torch.tensor(mask))\n",
    "  sentences = pad_sequence(sentences, batch_first=True, padding_value=bert_pad_token)\n",
    "  labels = torch.stack(labels, dim=0)\n",
    "  masks = pad_sequence(masks, batch_first=True, padding_value=0.0)\n",
    "  return sentences, labels, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f12667d-c06d-44d5-90e3-72dae98e00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes the amount of time that a training epoch took and displays it in human readable form\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1d8e75d-d1df-498c-8c91-686acdc1fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of trainable parameters in the model\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61789136-0db7-419c-96f7-2d693c5f87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a given model, using a pytorch dataloader, optimizer, and scheduler (if provided)\n",
    "def train(model,\n",
    "          dataloader,\n",
    "          optimizer,\n",
    "          device,\n",
    "          clip: float,\n",
    "          scheduler = None):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        sentences, labels, masks = batch[0], batch[1], batch[2]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(sentences.to(device), masks.to(device))\n",
    "        loss = F.cross_entropy(output, labels.to(device))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "          scheduler.step()\n",
    "          \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c1367d-9294-4444-8b7d-6c6f3a98f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the loss from the model on the provided dataloader\n",
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch in dataloader:\n",
    "          sentences, labels, masks = batch[0], batch[1], batch[2]\n",
    "          output = model(sentences.to(device), masks.to(device))\n",
    "          loss = F.cross_entropy(output, labels.to(device))\n",
    "            \n",
    "          epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14f0cda9-7401-4452-9c60-59622aebfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the prediction accuracy on the provided dataloader\n",
    "def evaluate_acc(model,\n",
    "                 dataloader,\n",
    "                 device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "      total_correct = 0\n",
    "      total = 0\n",
    "      for i, batch in enumerate(dataloader):\n",
    "          \n",
    "          sentences, labels, masks = batch[0], batch[1], batch[2]\n",
    "          output = model(sentences.to(device), masks.to(device))\n",
    "          output = F.softmax(output, dim=1)\n",
    "          output_class = torch.argmax(output, dim=1)\n",
    "          total_correct += torch.sum(torch.where(output_class == labels.to(device), 1, 0))\n",
    "          total += sentences.size()[0]\n",
    "\n",
    "    return total_correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1420bec-2726-4bc1-9077-1a080fa3c6c3",
   "metadata": {},
   "source": [
    "# Model Setup and Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "190965f5-3035-4722-9e94-b271a51c0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, install the hugging face transformer package in your colab\n",
    "# !pip install transformers\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "821519f9-dbe6-4e8c-82d7-1d8f46df1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Do not change this line, as it sets the model the model that Hugging Face will load\n",
    "# If you are interested in what other models are available, you can find the list of model names here:\n",
    "# https://huggingface.co/transformers/pretrained_models.html\n",
    "bert_model_name = 'distilbert-base-uncased' \n",
    "##YOUR CODE HERE##\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6cde0b-a913-4193-9ab9-a67d35105ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert_encoder: nn.Module,\n",
    "                 enc_hid_dim=768, #default embedding size\n",
    "                 outputs=2,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert_encoder = bert_encoder\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        \n",
    "        \n",
    "        ### YOUR CODE HERE ### \n",
    "        self.pre_classifier = nn.Linear(enc_hid_dim, enc_hid_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.out = nn.Linear(enc_hid_dim, outputs)\n",
    "\n",
    "    def forward(self,\n",
    "                src,\n",
    "                mask):\n",
    "        bert_output = self.bert_encoder(src, mask)\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        # embed = bert_output.last_hidden_state[:, 0]\n",
    "        embed = bert_output[0][:,0]\n",
    "        return self.out(self.dropout(self.relu(self.pre_classifier(embed))))\n",
    "    \n",
    "    def inference(self,\n",
    "                src,\n",
    "                mask):\n",
    "        bert_output = self.bert_encoder(src, mask)\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        # embed = bert_output.last_hidden_state[:, 0]\n",
    "        embed = bert_output[0][:,0]\n",
    "        return self.out(self.dropout(self.relu(self.pre_classifier(embed)))), embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6b996df-6e18-48ed-b22c-290959c3f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_classification_head_weights(m: nn.Module, hidden_size=768):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    k = 1/hidden_size\n",
    "    for name, param in m.named_parameters():\n",
    "        if name == 'out.weight' or name == 'pre_classifier.weight':\n",
    "            print(name)\n",
    "            nn.init.uniform_(param.data, a=-1*k**0.5, b=k**0.5)\n",
    "        elif name == 'out.bias' or name == 'pre_classifier.bias':\n",
    "            print(name)\n",
    "            nn.init.uniform_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfbda0fa-640d-44f1-8c8d-3a247e125a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized\n"
     ]
    }
   ],
   "source": [
    "#define hyperparameters\n",
    "BATCH_SIZE = 10\n",
    "LR = 1e-5 # 1e-4 # 1e-5\n",
    "WEIGHT_DECAY = 0\n",
    "N_EPOCHS = 5 #3\n",
    "CLIP = 1.0\n",
    "\n",
    "#define models, move to device, and initialize weights\n",
    "# device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:3')\n",
    "\n",
    "model = TweetClassifier(bert_model, outputs=len(mapping_l2i)).to(device)\n",
    "model.to(device)\n",
    "print('Model Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96321bc0-268a-403d-865b-007dab237d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pytorch dataloaders from train_dataset, val_dataset, and test_datset\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,collate_fn=partial(transformer_collate_fn, tokenizer=tokenizer), shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=BATCH_SIZE,collate_fn=partial(transformer_collate_fn, tokenizer=tokenizer))\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,collate_fn=partial(transformer_collate_fn, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45eb23e2-ccf2-45fe-bb25-06524e8b7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(dataloader, split='train', mode='pretrain'):\n",
    "    embeddings, pred, gtlabels = [], [], []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            sentences, labels, masks = batch[0], batch[1], batch[2]\n",
    "            output, emb = model.inference(sentences.to(device), masks.to(device))\n",
    "            embeddings.append(emb.cpu().detach())\n",
    "            pred.append(output.cpu().detach())\n",
    "            gtlabels.append(labels.cpu().detach())\n",
    "    \n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    pred = torch.cat(pred, dim=0)\n",
    "    gtlabels = torch.cat(gtlabels)\n",
    "    \n",
    "    print(embeddings.shape, pred.shape, gtlabels.shape)\n",
    "    \n",
    "    torch.save(embeddings, f'emb/{dataset}_{split}_{mode}_emb.pt')\n",
    "    torch.save(pred, f'emb/{dataset}_{split}_{mode}_pred.pt')\n",
    "    torch.save(gtlabels, f'emb/{dataset}_{split}_{mode}_gt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04cc06bc-1f56-4c18-9ee3-b82a3359cad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1267, 768]) torch.Size([1267, 6]) torch.Size([1267])\n",
      "torch.Size([1284, 768]) torch.Size([1284, 6]) torch.Size([1284])\n",
      "torch.Size([10240, 768]) torch.Size([10240, 6]) torch.Size([10240])\n"
     ]
    }
   ],
   "source": [
    "save_embedding(test_dataloader, 'Test')\n",
    "save_embedding(val_dataloader, 'Validation')\n",
    "save_embedding(train_dataloader, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae3377e0-4e75-4344-9d92-2a31cd47ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 66,955,010 trainable parameters\n",
      "Initial Train Loss: 0.702\n",
      "Initial Train Acc: 0.476\n",
      "Initial Valid Loss: 0.701\n",
      "Initial Valid Acc: 0.477\n",
      "Epoch: 01 | Time: 0m 48s\n",
      "\tTrain Loss: 0.235\n",
      "\tTrain Acc: 0.972\n",
      "\tValid Loss: 0.152\n",
      "\tValid Acc: 0.958\n",
      "Epoch: 02 | Time: 0m 49s\n",
      "\tTrain Loss: 0.101\n",
      "\tTrain Acc: 0.989\n",
      "\tValid Loss: 0.120\n",
      "\tValid Acc: 0.970\n",
      "Epoch: 03 | Time: 0m 48s\n",
      "\tTrain Loss: 0.054\n",
      "\tTrain Acc: 0.994\n",
      "\tValid Loss: 0.124\n",
      "\tValid Acc: 0.974\n",
      "Epoch: 04 | Time: 0m 49s\n",
      "\tTrain Loss: 0.030\n",
      "\tTrain Acc: 0.998\n",
      "\tValid Loss: 0.135\n",
      "\tValid Acc: 0.971\n",
      "Epoch: 05 | Time: 0m 45s\n",
      "\tTrain Loss: 0.019\n",
      "\tTrain Acc: 0.998\n",
      "\tValid Loss: 0.143\n",
      "\tValid Acc: 0.972\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=10, num_training_steps=N_EPOCHS*len(train_dataloader))\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "train_loss = evaluate(model, train_dataloader, device)\n",
    "train_acc = evaluate_acc(model, train_dataloader, device)\n",
    "\n",
    "valid_loss = evaluate(model, val_dataloader, device)\n",
    "valid_acc = evaluate_acc(model, val_dataloader, device)\n",
    "\n",
    "print(f'Initial Train Loss: {train_loss:.3f}')\n",
    "print(f'Initial Train Acc: {train_acc:.3f}')\n",
    "print(f'Initial Valid Loss: {valid_loss:.3f}')\n",
    "print(f'Initial Valid Acc: {valid_acc:.3f}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_dataloader, optimizer, device, CLIP, scheduler)\n",
    "    end_time = time.time()\n",
    "    train_acc = evaluate_acc(model, train_dataloader, device)\n",
    "    valid_loss = evaluate(model, val_dataloader, device)\n",
    "    valid_acc = evaluate_acc(model, val_dataloader, device)\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tTrain Acc: {train_acc:.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f}')\n",
    "    print(f'\\tValid Acc: {valid_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4237b273-6699-42a0-96be-e730a4c1b75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'model/{dataset}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1770ca2-c416-4508-a7a0-b36b96bffe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.721\n",
      "Test Acc: 0.275\n"
     ]
    }
   ],
   "source": [
    "#run this cell and save its outputs to receive full credit for this implementation\n",
    "test_loss = evaluate(model, test_dataloader, device)\n",
    "test_acc = evaluate_acc(model, test_dataloader, device)\n",
    "print(f'Test Loss: {test_loss:.3f}')\n",
    "print(f'Test Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8cd6d07-494d-4aca-bc21-e37b54cbd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'model/{dataset}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c84591ca-d4ca-46c3-9dd8-c92d849cce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1267, 768]) torch.Size([1267, 6]) torch.Size([1267])\n",
      "torch.Size([1284, 768]) torch.Size([1284, 6]) torch.Size([1284])\n",
      "torch.Size([10240, 768]) torch.Size([10240, 6]) torch.Size([10240])\n"
     ]
    }
   ],
   "source": [
    "save_embedding(test_dataloader, 'Test', 'finetune')\n",
    "save_embedding(val_dataloader, 'Validation', 'finetune')\n",
    "save_embedding(train_dataloader, 'Train', 'finetune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b14b6c-76c0-4c21-8b95-665f4ebb7675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
